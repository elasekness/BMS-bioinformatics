## Tutorial 2

**Objective:** To perform reference-based assemblies of SARS-CoV-2 genomes to determine whether cases from patients and staff at a facility are part of the same
transmission chain.

You will map reads to a reference assembly, generate consensus genomes, identify single nucleotide variants (SNVs),
and phylogenetically place genomes to determine whether the cases in question are epidemiologically related.
Additionally, we will employ and compare the results of different methods to generate our consensus genomes.
This includes using software that I have pre-installed on the VM as well as containerized versions of the same
software pulled from Docker. Finally, you will learn about GitHub and Docker for obtaining and installing bioinforamtics software.
<br>

## Some general information

Our read data are in fastq format. Fastq files contain the quality score for each nucleotide in the read and each read has four lines associated with it:

	@SRR10971381.1 1 length=151
	AGTCGATCAGCTGAGTACTAGTAGCATG
	+
	BBBCCCKKIBCCKKBIJDFFHHHIIJKK

> The read name, naming conventions may vary but names are always prefaced by '@'<br>
> This is followed by the sequence line <br>
> `+` a Line break <br>
> Followed by quality scores for each nucleotide in ASCII format

<br>

Each base has an associated quality score, which indicates the reliability of that call.
Phred scores are equal to -10 log<sub>10</sub>P, where P is the error probability for the base in question.
Thus, a phred score of 10 is equal to an error probability of 0.01 (P=10<sup>–Q/10</sup>) or a 1 in 10 chance that the base call is incorrect.

Most NGS output is in fastq format.  We are working with short, paired-end reads (2x250 bp or 2x150 bp)
generated by Illumina sequencers (in this case a MiSeq or a NextSeq).
While Illumina reads are shorter than those from traditional Sanger sequencing, Illumina sequencing can produce millions to hundreds of millions of reads per sample.
<br>


## General workflow for producing a reference-based assembly

Reference-based assemblies rely on a reference genome on which to map the reads of a genomic library.
Reference assemblies allow the direct comparison of a genome of interest to an already-assembled genome for SNP and/or Indel detection.
Not all differences detected will be legitimate variation.  Miss-assembly (usually due to repetitive elements),
poor quality reads, low coverage, and contamination can all contribute to erroneous base calls.
Implementing additional filtering criteria to detect true variation is necessary,
such as only considering SNPs from an area of high coverage with a certain number of high-quality reads.
A general workflow for generating a reference-based assembly is given below:

<br>

**Quality control**

  - Generate some summary statistics with FastQC

**Adaptor removal (and primer removal) and quality trimming**

 - cutadapt
 - Trimmomatic
 - BBduk from BBtools
 - FastX-toolkit
 - TrimGalore
 - Seqyclean
 - And many more

**Read correction (optional, this is more important for long-read sequencing technologies such as Nanopore)**

 - Quake
 - BBnorm
 - SHRIMP
 - Quorum
 - Lighter
 - Fermi
 - Musket
 - And many, many more

**Read alignment or mapping to reference genome**

 - BWA
 - Bowtie
 - BBmap
 - STAR (for RNA-Seq)

**General statistics pertaining to reference assembly**

 - Samtools
 - Picard
 - BBtools
 - Quast

**SNP and INDEL calling**

 - Samtools/bcftools
 - GATK
 - iVAR

**Additional SNV filtering and annotation**

In our case, we'll also be generating consensus genomes with iVAR

<br>

## Background information for SARS-CoV-2 cases under investigation
<br>
In early March 2020 of the pandemic, healthcare facilities struggled with outbreaks of COVID-19.
Genomic epidemiology can assist investigations into the source of an outbreak, the timing of introduction
of a pathogen into a population, and the number of introductions that have occurred into an area.
Here we are using the information contained within SARS-CoV-2 genomes to assist a traditional epidemiological investigation
into the nature of an outbreak in a Westchester healthcare facility. The are four samples from the facility: two
from staff and two from residents, collected on the same day. A fifth sample is also from a healthcare facility collected four days after.
While genomic epidemiology cannot confirm the individual source
in this case, we want to know if the virus was transmitted within the facility and if all cases are a part of the same outbreak.
Keep in mind that the mutation rate for SARS-CoV-2 is about 1 mutation every two weeks.  At the beginning of the pandemic,
there was very little variation in SARS-CoV-2 genomes.  Thus, one unique mutation shared among several genomes that is not observed
in a contextual set of genomes might provide support for a transmission link.  Conversely, it could be that there are many unsampled
genomes, some of which also contain the unique mutation.

<br>

Each student will map the fastqs from one sample to the reference genome and generate a consensus genome for downstream analyses.

## Sample assignment and metadata

| Student | Collection date | Sample name | County | Healthcare facility |
| ------- | --------------- | ----------- | ------ | ------------------- |
| Student1 | 2020-03-15 | IDR1 | Westchester | Yes, staff |
| Student2 | 2020-03-15 | IDR2 | Westchester | Yes, staff |
| Student3 | 2020-03-15 | IDR3 | Westchester | Yes, resident |
| Student4 | 2020-03-15 | IDR4 | Westchester | Yes, resident |
| Student5 | 2020-03-19 | IDR5 | Westchester | Yes, resident |



## Follow the general workflow outlined previously to process and map your reads and generate a consensus genome

<br>

Use the **`gsutil`** copy command to copy the fastq files for your sample from our bucket to the terminal.

Before we can copy to and from our bucket, we need to authenticate our accounts to GCP on the computer from which we are executing the gsutil commands.

	gcloud auth login

> A crazy long URL is output. Copy the URL from the terminal window and paste it into a browser (preferably Chrome).
> The web page that is displayed will contain a long auth code. Copy the code and paste it back into the terminal window
> you executed the gcloud cmd in. press return and you should be authenticated to GCP. Some current state info will be output.

<br>

	gsutil cp gs://wc-bms-bi-training-bucket/outbreak_fastqs/IDR1*gz .
	
> Make sure you substitute your IDR number in the command above. The ending period is important! Remember, you must specify a destination for your copied files, 
> which is you current working directory in this case.

<br>

Clean your raw reads and remove any remaining adapters with TrimGalore, where fastq_R1 and fastq_R2 are your fastq files.

	trim_galore -q 20 --length 100 --paired fastq_R1 fastq_R2
	
> Adapters are short oligonucleotides ligated to a library for sequencing on Illumina machines. These are typically removed by the Sequencing Core but some 
> may still remain. We might also want to remove reads that fall below a certain average quality or length.
> Before running trim_galore, explore the options available to you with the help command. Most default setting are fine.
>
> Here we are using the default quality threshold of 20 (so we don't even have to specify it) and a minimum sequence length of 100. 
> **`Paired`** keeps R1 and R2 reads in order.  In other words, if one of the pairs fails quality control, both are removed.  
> The nice thing about TrimGalore is it pretty much does everything for you automatically, including detecting the type of adapter present 
> It also prints some nice summary statistics to STDOUT.

* Judging from the TrimGalore output, does your sequencing run look good?
* What is the percentage of reads that passed trimming?
* You can also look at the quality with [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)

<br>

Create an index of your reference genome.

Your adapter, quality trimmed reads are now in files with a "val_[1,2].fq.gz" ending. We'll map these to the Wuhan-1 reference genome.
Before we perform the read alignment, we need to index the reference assembly. You might want to rename your fna file to something shorter for easier
viewing of long commands in your terminal window.  I've renamed mine "wuhan.fna."

	bwa index wuhan.fna

> [BWA](http://bio-bwa.sourceforge.net">http://bio-bwa.sourceforge.net) (Burrows-Wheeler Aligner) is a short-read aligner tool.
> Most short-read aligners rely on breaking reads into K-mers (shorter sequences of a specified length),
> aligning these to the reference genome, and extending these seeds with some amount of base misincorporation.
> This is actually faster than aligning the entire read. BWA outputs the alignment information in a [SAM](https://samtools.github.io/hts-specs/SAMv1.pdf) format.
> The output from BWA would need to be saved in a file, converted to a binary format, and sorted by read position for additional analyses,
> such as SNP calling or consensus genome building.  While we could do each step individually, we can also pipe these commands together for faster processing
> and to avoid generating intermediate files that we would later delete.

<br>

Align your reads to the reference genome with BWA, pipe the output to samtools to sort the reads by their coordinates and to convert to a binary format.

	bwa mem reference_assembly/wuhan.fna val_1.fq.gz val_2.fq.gz | samtools sort | samtools view -F 4 -o IDRnumber.sorted.bam

> Here we are piping output to **`samtools`** twice - once to sort the reads and the next time to convert the output to binary format (SAM to BAM) and save
> that to a file with a '.sorted.bam' ending. The **`-F 4`** flag specifies that samtools should not write unaligned reads to the bam file.

<br>

Check the alignment quality with samtools 'flagstat' option.

	samtools flagstat IDRnumber.sorted.bam

> This command will output some summary statistics for your alignment.  Notice that 100% of your reads aligned to the reference assembly
> because we excluded unaligned reads from the final sorted bam file.
> Does this seem like a good alignment to you based on the number of PE (paired-end) reads that aligned and are properly paired with each other?

<br>

Remove primer sequences from the alignment with iVAR.

The reads you have just processed were generated with a modified [ARTIC protocol](https://artic.network/ncov-2019)
which, amplifies viral cDNA using a set of tiled, multiplexed primers.  These primers can introduce
bias in base calling and subsequently affect variant identification and our consensus genome if we don't remove them.
[iVAR](https://github.com/andersen-lab/ivar) is a tool that will
remove primers from a BAM alignment, call variants, and produce a consensus genome.
The primer removal process requires a bed file that contains the locations of the primers
(a bed file is a tab delimited file that contains genomic coordinates for regions of interest).
We'll first need to obtain the bed file from our bucket. We'll then need to use Docker to run iVAR.

	gsutil cp gs://wc-bms-bi-training-bucket/reference_assembly/nCoV-2019.V3.ivar.bed .
	docker run --rm -v $(pwd):/data -w /data staphb/ivar ivar trim -i IDRnumber.sorted.bam -b nCoV-2019.V3.ivar.bed -e -p IDRnumber

> **`--rm`** removes container after use, **`-v`** mounts your directory into the container, **`-w`** allows the command being executed to access your mounted 
> directory, **`staphb/ivar`** is the container to be run (note that the repository/program must be specified), **`ivar trim`** is the command being executed. 
> The **`-i`**, **`-b`**, **`-e`**, and **`-p`** arguments are specific to the **`ivar trim`** command.
>
> You can also run iVar within the container (interactively) to explore the different command options with **`docker run -it staphb/ivar`**

<br>

iVAR prints some nice summary statistics to STDOUT.  You can see how many reads were generated by each primer, the percentage of reads that were trimmed and
removed due to insufficient length.  Ideally, we would like to visually check that our alignment looks ok and primer-free with
[IGV](https://software.broadinstitute.org/software/igv/home).  If we have time, we can install IGV on your computers and download the files we need from GCP to view our results.

<br>

Notice that iVAR outputs a BAM file named with your specified prefix and the '.bam' ending.  We must sort the reads again.

	samtools sort IDRnumber.bam -o IDRnumber.sorted2.bam

<br>

Call variants with bcftools and iVar.

Let's use two different tools to detect SNVs (single nucleotide variants).

	bcftools mpileup -f reference_assembly/wuhan.fna IDRnumber.sorted2.bam | bcftools call -c -o IDRnumber.vcf
	
> bcftools will output the base call at each position in the genome.  Use your Linux skills to parse the file and find regions of variation.

<br>

	samtools mpileup -aa --reference wuhan.fna IDRnumber.sorted2.bam | docker run -i --rm -v $(pwd):/data -w /data staphb/ivar ivar variants -p IDRnumber_ivar
	
> The output of **`samtools mpileup`** must be passed to **`iVAR`** in order to call variants.  The **`-i`** flag tells the Docker container to
take information piped to it from STDOUT. Make sure that the parameters values are the ones you want by examining the parameter options of each command.

* What does the output of samtools mpileup look like?
* Do you get the same results with both tools?
* Parsing your results with your command-line skills will make it fairly quick to answer the last question.

<br>

Make a consensus genome with iVar.

	samtools mpileup -aa --reference wuhan.fna IDRnumber.sorted2.bam | docker run -i --rm -v $(pwd):/data -w /data staphb/ivar ivar consensus -t 0.75 -m 50 -t 0.90 -m 50 -n N -p IDRnumber

<br>

## Evaluate whether your genomes represent viruses from the same transmission chain
<br>

Now that we have our consensus genomes, we'll want to compare them to others from the facility outbreak as well as to those from NYS and even other states/countries from the same time period. A good place to start is to create a multi-sequence alignment, which you will use to generate a phylogeny.  The phylogeny will help you visualize the relationships among your genomes. We can create SNP matrix as well. This would allow us to quickly see how different our genomes are from each other although information regarding relationships is lost.

This is a good opportunity to become familiar with GitHub - a repository for code, workflows, and associated data. GitHub is the source for much of the software we are using. There are often many ways to install software and the type of installation you perform will depend on your operating system (if Linux – the flavor of Linux you are using), what other dependencies you have installed or need installed (including a compiler), and the options for installation provided by the developer. Some software can be downloaded as a pre-compiled binary, which should require no further steps from you to work if you have downloaded the appropriate binary for your OS. And many bioinformatics programs can be installed with the apt-get commands if you are working in Ubuntu (we are!).  For example, 'sudo apt-get install prokka' would search a database of available packages and install the annotation program 'prokka' as well as all of its many dependencies, and put the software in the appropriate place on our VM (in our path). The caveat is you that need sudo privileges to install the software system-wide (and not just locally on your account).

<br>

Go to the GitHub page for the program [snp-dists](https://github.com/tseemann/snp-dists), which we will use to calculate the number of SNPs between sequences.

Click on the green 'Code' button and copy the link 'https://github.com/tseemann/snp-dists.git'

In your terminal window type:

	git clone https://github.com/tseemann/snp-dists.git
	cd snp-dists
	make

* What did this do?

> You should see that an executable called 'snp-dists' was compiled by the 'make' command.  Executables are conveniently 
> colored green on our VM. We could move this to a place that's in our PATH (such as /usr/local/bin) so that we'd only have 
> to type the `snp-dists` command when we want to run the program but this also requires sudo privileges 
> (and the executable is already there so don't try this).  
> Or we can simply point the VM to the location of our snp-dists program when we want to run it.

<br>

Copy your consensus genomes to the bucket.  Once everyone has copied their fasta files, copy all of them back to your VM.

	gsutil cp IDRnumber.fa gs://wc-bms-bi-training-bucket/consensus_genomes
	gsutil gs://wc-bms-bi-training-bucket/consensus_genomes/*fa .

> Note that there was an additional fasta file in this bucket called 'additional_genomes.fa' that you have copied over.  
> These are contextual sequences to help us determine whether the genomes from our investigation are linked.
> The contextual sequences were collected from the same time period as our genomes, from the same or neighboring
> counties as where our specimens were collected. These genomes were obtained from [GISAID](https://www.gisaid.org/),
> which has become the global standard for depositing SARS-CoV-2 genomes and associated metadata.
> 
> Since you are copying all fasta files, the copy of your fasta file in your account will simply be overwritten.

<br>

Concatenate your files into a single multi-fasta file and align your genomes with [MAFFT](https://mafft.cbrc.jp/alignment/software/)

	cat *fa > all.fasta
	mafft all.fasta > all.aln.fasta
	
> If we had thousands of genomes to align, it would be more efficient to align each to a guide genome.

	mafft --auto --keeplength --addfragments all.fasta wuhan.fna > all.aln.fasta

<br>

Reconstruct a phylogeny with your alignment 'all.aln.fasta' in IQTree.

	iqtree -s all.aln.fasta -m GTR+G4
	
> [IQTree](http://www.iqtree.org/) is a program for rapidly generating a maximum likelihood (ML) tree for hundreds to thousands of sequences. There are several methods to reconstruct a phylogeny - Parsimony, Neighbor-Joining, Maximum Likelihood, and Bayesian. ML methods rely on models of nucleotide or amino acid substitution.  There are many models - from simple to complex - and ideally we would want to test them all to see which fit our data best.  This can also be done with IQTree (see documentation or help page). Theoretically, an ML program would test every single possible tree given our data and find the one that maximizes the likelihood that our evolutionary model generated the data. However, the number of trees to be tested becomes inordinately large as you increase the number of taxa in your tree. Thus, IQTree employs some shortcut methods (which we won't cover here) so as not to test every tree.

> The `-s` option specifies the input alignment and the `-m` option specifies the model.  We are using the general-time-reversible (GTR) [susbtitution model](https://evomics.org/resources/substitution-models/nucleotide-substitution-models/) and a gamma distribution (G) with four rate categories to take into account that different positions may evolve at different rates.

<br>

View the phylogeny ('all.aln.fasta.treefile') with [FigTree](http://tree.bio.ed.ac.uk/software/figtree/) or [Seaview](http://doua.prabi.fr/software/seaview).

> We'll download these programs to your computers and learn how to import tree files and associated metadata in class. 
> We'll also demo how to download a file from your VM and the metadata file in the 'consensus_genomes' bucket from your GCP console to your computer.

* Do you think these genomes are part of the same transmission chain?
* Which cases does the phylogeny support as being linked or not linked?

<br>

Generate a SNP matrix from you final alignment with your installed version of `snp-dists`.

	snp-dists/snp-dists all.aln.fasta > snp_matrix.txt
	
> Remember, since the `snp-dists` program isn't in a place that your VM looks to find executables, you have to specify the path to where your version is located. 

A maxtrix is rather cumbersome to visualize.  Try the `-m` or molten option of `snp-dists`, which prints the pairwise genome comparisons as a tab delimited list.

	snp-dists/snp-dists -m all.aln.fasta > snp_matrix.txt
	
* Parse this file to find your genomes of interest compared to each other. 
* Parse this file to find the average SNP difference among your genomes and all genomes.
* Does this help you reach a conclusion about whether these cases are linked? Why or why not?

## Extra: Redo the analysis above employing Dockerized versions of the software

List the Docker images already on your VM that you can use for this exercise.

	docker images

> You can run the dockerized version of TrimGalore to produce trimmed reads again or redo the analysis starting with the previously generated trimmed reads.
> With trimmed reads in hand, you can either perform each task of the workflow sequentially or you can string together the commands in a pipe, as we did with
the compiled software.  To indicate that the Docker image should use the output from the former command as input, we use the **`i`** flag. 
> As usual, in the example below, substitute "IDRnumber" with the name of your files.

<br>

## Docker

A Docker container image is essentially like a little VM that contains code for an application and all of its dependencies.
Thus, Docker containers should run the same way on any Linux machine. This is valuable because software is often
difficult to compile from source code.  Docker containers also eliminate issues with versioning, such that you can
have and run docker containers of multiple versions of the same program without issue. Conveniently, Docker has already
been installed for us on our machines.

Docker images can be 'pulled' from repositories on [Docker hub](https://hub.docker.com/) 
Anyone with a Docker account can create a repository for software that they have containerized.
Two repositories that I pull from frequently are [staphb](https://github.com/StaPH-B/docker-builds) and 'biocontainers.'
Although I'm using the terms Docker image and Docker container synonymously, an image is really the instructions for making the container. The image
creates the VM instance.

Two commands that you will use are **`docker pull`** and **`docker run`**

The command: **`docker pull staphb/ivar`**, will pull the latest image of iVar from the staphb repository registered on Docker Hub.
You can specify another version with tags: **`docker pull staphb/spades:3.12.0`**, now pulls an image for spades version 3.12.0
<br>
The **`docker run`** command will:
* initiate Docker client talking to docker daemon
* pull image if not found locally by Docker daemon
* start new container from image
* execute specified command
<br>
The docker run command essentially converts our image to a container.  To actually execute our command
on our files, we need to make them visible to the container.  Thus, we must mount a working directory inside the VM.  To do this, we
will use two arguments with our run command:

**`-v $(pwd):/data`** mounts our current directory to the VM (it will also create a directory called 'data' if it doesn't exist).
**`-w /data`** allows the command being executed to run in the directory specified.

When the container runs, it's being run as the 'root' user. We can change this but the default is fine for now.
The only issue is that the output produced will be owned by 'root', not by us.  Thus, we will have read-only permissions.  This shouldn't generally be a problem
because we aren't altering most of our output files, but we can always change the permissions with a **`sudo chmod`** command.

<br>

For an additional Docker tutorial see [https://github.com/PawseySC/bio-workshop-18](https://github.com/PawseySC/bio-workshop-18)
